{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0130fa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Conv2D, MaxPooling2D, UpSampling2D, Input, Rescaling, BatchNormalization, Reshape, Flatten\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e7909b",
   "metadata": {},
   "source": [
    "# Anemia prediction (Classification & Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ebbb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"classification\"\n",
    "#mode = \"regression\"\n",
    "binary = True\n",
    "threshold = [7.0, 10.0, 12.5]\n",
    "threshold_name = [\"severely anemic\", \"moderately anemic\", \"mildly anemic\", \"non-anemic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e209f854",
   "metadata": {},
   "source": [
    "## Load labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5de43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = os.path.join(\"D:\", \"OneDrive_1_5-26-2022\", \"PredictingAnemia_DATA_2022-06-05_0643.csv\")\n",
    "label = pd.read_csv(label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f3fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label[\"hgb\"] = pd.to_numeric(label[\"hgb\"], errors=\"coerce\")\n",
    "drop_index = np.where(pd.isnull(label[\"hgb\"]))\n",
    "print(\"drop (contains string or null): \", drop_index[0])\n",
    "label = label.drop(drop_index[0])\n",
    "print(\"mean:\", label[\"hgb\"].mean(), \"std:\", label[\"hgb\"].std())\n",
    "print(\"anemia mean: \", label[\"hgb\"][label[\"hgb\"] < 12.5].mean())\n",
    "print(\"non-anemia mean: \", label[\"hgb\"][label[\"hgb\"] >= 12.5].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9d53cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_class_label(label_i, threshold):\n",
    "    label = -1\n",
    "    for i, threshold_i in enumerate(threshold):\n",
    "        if label_i < threshold_i:\n",
    "            label = i\n",
    "            break\n",
    "    if label == -1:\n",
    "        label = len(threshold)\n",
    "    \n",
    "    #print(label, label_i)\n",
    "    \n",
    "    return label\n",
    "\n",
    "if mode == \"classification\":\n",
    "    if binary:\n",
    "        y = (label[\"hgb\"] < 12.5).astype(int)\n",
    "    else:\n",
    "        y = np.array([multi_class_label(label_i, threshold) for label_i in label[\"hgb\"]], dtype=np.uint8)\n",
    "        y = pd.Series(data=y, index=label[\"hgb\"].index)\n",
    "elif mode == \"regression\":\n",
    "    y = label[\"hgb\"]\n",
    "print(y.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc324d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_available = [] \n",
    "\n",
    "for folder in os.listdir(\"./detected eyes images\"):\n",
    "    if int(folder)-1 in y.index:\n",
    "        y_available.append(int(folder))\n",
    "        \n",
    "print(\"not available id: \")\n",
    "not_available_id = []\n",
    "for i in range(1, 693):\n",
    "    if i not in y_available:\n",
    "        not_available_id.append(i)\n",
    "print(not_available_id)\n",
    "print(\"num: \", len(not_available_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419b3320",
   "metadata": {},
   "source": [
    "## Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf44e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_img = []\n",
    "y_img = []\n",
    "y_id = []\n",
    "\n",
    "for id in y_available:\n",
    "    for image in os.listdir(os.path.join(\"./detected eyes images\", str(id))):\n",
    "        #print(id, image)\n",
    "        img = cv2.imread(os.path.join(\"./detected eyes images\", str(id), image))\n",
    "        #print(img.shape)\n",
    "        x_img.append(tf.image.resize(img, (224, 224)))\n",
    "        y_img.append(y[id-1])\n",
    "        y_id.append(id)\n",
    "        \n",
    "x_img = np.array(x_img, dtype=np.uint8)\n",
    "y_img = np.array(y_img)\n",
    "print(x_img.shape, y_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552220fd",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce77c67",
   "metadata": {},
   "source": [
    "### U-Net and autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dd5f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hsv_hist = np.load('./x_hsv_hist.npz')[\"x\"]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_hsv_hist = scaler.fit_transform(x_hsv_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d594fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(array):\n",
    "    \"\"\"\n",
    "    Normalizes the supplied array and reshapes it into the appropriate format.\n",
    "    \"\"\"\n",
    "\n",
    "    array = array.astype(\"float32\") / 255.0\n",
    "    array = np.reshape(array, (len(array), 224, 224, 3))\n",
    "    return array.astype(np.float32)\n",
    "\n",
    "def noise(array):\n",
    "    \"\"\"\n",
    "    Adds random noise to each image in the supplied array.\n",
    "    \"\"\"\n",
    "\n",
    "    noise_factor = 0.4\n",
    "    noisy_array = array + noise_factor * np.random.normal(\n",
    "        loc=0.0, scale=1.0, size=array.shape\n",
    "    )\n",
    "\n",
    "    return np.clip(noisy_array, 0.0, 1.0)\n",
    "\n",
    "def display(array1, array2):\n",
    "    \"\"\"\n",
    "    Displays ten random images from each one of the supplied arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    n = 10\n",
    "\n",
    "    indices = np.random.randint(len(array1), size=n)\n",
    "    \n",
    "    images1 = np.zeros_like(array1[indices, :])\n",
    "    images2 = np.zeros_like(array2[indices, :])\n",
    "    images1[:, :, :, 0] = array1[indices, :, :, 2]\n",
    "    images1[:, :, :, 1] = array1[indices, :, :, 1]\n",
    "    images1[:, :, :, 2] = array1[indices, :, :, 0]\n",
    "    images2[:, :, :, 0] = array2[indices, :, :, 2]\n",
    "    images2[:, :, :, 1] = array2[indices, :, :, 1]\n",
    "    images2[:, :, :, 2] = array2[indices, :, :, 0]\n",
    "\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i, (image1, image2) in enumerate(zip(images1, images2)):\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(image1)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(image2)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "x_img_preprocess = preprocess(x_img)\n",
    "x_img_noise = noise(x_img_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fbaa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vgg16(tf.keras.Model):\n",
    "    def __init__(self, pretrained = True):\n",
    "        super(Vgg16, self).__init__()\n",
    "        self.vggnet = tf.keras.applications.VGG16(include_top=False, weights=None)\n",
    "        \n",
    "    def call(self, x):\n",
    "        results = []\n",
    "        for ii,model in enumerate(self.vggnet.layers):\n",
    "            x = model(x)\n",
    "            if ii in [2,5,9,13,17]:\n",
    "                results.append(x) #(64,256,256),(128,128,128),(256,64,64),(512,32,32),(512,16,16)\n",
    "        return results\n",
    "\n",
    "vgg_model = Vgg16()\n",
    "vgg_model.build(input_shape=(None, 224, 224, 3))\n",
    "vgg_model.summary()\n",
    "\n",
    "class DeConv2d(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, stride, padding, dilation):\n",
    "        super().__init__()\n",
    "        self.up = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')\n",
    "        self.conv = tf.keras.layers.Conv2D(filters=out_channel, kernel_size=kernel_size, strides=stride, padding=padding, dilation_rate=dilation)\n",
    "    \n",
    "    def call(self, x):\n",
    "        output = self.up(x)\n",
    "        output = self.conv(output)\n",
    "        return output\n",
    "\n",
    "class UNet(tf.keras.Model):\n",
    "    def __init__(self, pretrained_net, n_class):\n",
    "        super().__init__()\n",
    "        self.n_class = n_class\n",
    "        self.pretrained_net = pretrained_net\n",
    "        #####################################\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.deconv1 = DeConv2d(512, 512, kernel_size=3, stride=1, padding=\"same\", dilation=1)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.deconv2 = DeConv2d(1024, 256, kernel_size=3, stride=1, padding=\"same\", dilation=1)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.deconv3 = DeConv2d(512, 128, kernel_size=3, stride=1, padding=\"same\", dilation=1)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.deconv4 = DeConv2d(256, 64, kernel_size=3, stride=1, padding=\"same\", dilation=1)\n",
    "        self.bn4 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.classifier = tf.keras.layers.Conv2D(n_class, kernel_size=1, activation=\"sigmoid\")\n",
    "        #####################################\n",
    "    \n",
    "    def call(self, x):\n",
    "        #####################################\n",
    "        pre_output = self.pretrained_net(x)\n",
    "        output = self.bn1(self.relu(self.deconv1(pre_output[4]))) #(512,32,32)\n",
    "        output = self.bn2(self.relu(self.deconv2(tf.concat([output, pre_output[3]], axis=-1)))) #(256,64,64)\n",
    "        output = self.bn3(self.relu(self.deconv3(tf.concat([output, pre_output[2]], axis=-1)))) #(128,128,128)\n",
    "        output = self.bn4(self.relu(self.deconv4(tf.concat([output, pre_output[1]], axis=-1)))) #(64,256,256)\n",
    "        output = self.classifier(tf.concat([output, pre_output[0]], axis=-1))\n",
    "        return output\n",
    "        #####################################\n",
    "        \n",
    "seg_model = UNet(pretrained_net=vgg_model, n_class=3)\n",
    "seg_model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "seg_model.build(input_shape=(None, 224, 224, 3))\n",
    "seg_model.summary()\n",
    "\n",
    "seg_model.fit(\n",
    "    x=x_img_preprocess,\n",
    "    y=x_img_preprocess,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "x_img_denoise = seg_model.predict(x_img_preprocess, batch_size=16)\n",
    "display(x_img_preprocess, x_img_noise)\n",
    "display(x_img_preprocess, x_img_denoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471eacb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.conv1 = Conv2D(256, (3, 3), activation='relu', padding='same')\n",
    "        self.max1 = MaxPooling2D((2, 2), padding='same')\n",
    "        self.batch1 = BatchNormalization()\n",
    "        self.conv2 = Conv2D(256, (3, 3), activation='relu', padding='same')\n",
    "        self.max2 = MaxPooling2D((2, 2), padding='same')\n",
    "        \n",
    "        # Embeddings\n",
    "        self.ave1 = GlobalAveragePooling2D()\n",
    "        self.dropout = Dropout(0.1)\n",
    "        \n",
    "        # Decoder\n",
    "        self.conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')\n",
    "        self.up3 = UpSampling2D((2, 2))\n",
    "        self.conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')\n",
    "        self.up4 = UpSampling2D((2, 2))\n",
    "        self.conv5 = Conv2D(3, (3, 3), activation='sigmoid', padding='same')\n",
    "        \n",
    "    def call(self, x, training):\n",
    "        x_img, x_hist = x\n",
    "        x = self.conv1(x_img)\n",
    "        x = self.max1(x)\n",
    "        x = self.batch1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.max2(x)\n",
    "        \n",
    "        #embeddings = self.dropout(x_hist, training=training) + self.ave1(x)\n",
    "        embeddings = x_hist + self.ave1(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.up3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.up4(x)\n",
    "        x = self.conv5(x)\n",
    "        return embeddings, x\n",
    "\n",
    "autoencoder = Autoencoder()\n",
    "autoencoder.build(input_shape=[(None, 224, 224, 3), (None, 256)])\n",
    "autoencoder.summary()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "bce_loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "bce_loss_metric = tf.keras.metrics.Mean()\n",
    "mse_loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices({\"img\": x_img_preprocess, \"hist\": x_hsv_hist})\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(16)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    #print(\"Start of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "    bce_loss_metric.reset_state()\n",
    "    mse_loss_metric.reset_state()\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        x_batch_img, x_batch_hsv_hist = x_batch_train[\"img\"], x_batch_train[\"hist\"]\n",
    "        with tf.GradientTape() as tape:\n",
    "            embeddings, reconstructed = autoencoder([x_batch_img, x_batch_hsv_hist], training=True)\n",
    "            # Compute reconstruction loss\n",
    "            loss = bce_loss_fn(x_batch_img, reconstructed) + mse_loss_fn(x_batch_hsv_hist, embeddings) * 10\n",
    "\n",
    "        grads = tape.gradient(loss, autoencoder.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
    "\n",
    "        bce_loss_metric.update_state(bce_loss_fn(x_batch_img, reconstructed))\n",
    "        mse_loss_metric.update_state(mse_loss_fn(x_batch_hsv_hist, embeddings) * 10)\n",
    "        \n",
    "    print(\"epoch %d: bce mean loss = %.4f, mse mean loss = %.4f, elapsed time: %ds\" % (epoch, bce_loss_metric.result(), mse_loss_metric.result(), time.time()-start_time))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices({\"img\": x_img_preprocess, \"hist\": x_hsv_hist})\n",
    "test_dataset = test_dataset.batch(16)\n",
    "    \n",
    "x_img_denoise = None\n",
    "x_embeddings = None\n",
    "for x_batch_test in test_dataset:\n",
    "    x_batch_img, x_batch_hsv_hist = x_batch_test[\"img\"], x_batch_test[\"hist\"]\n",
    "    output_embeddings, output_x = autoencoder([x_batch_img, x_batch_hsv_hist], training=False)\n",
    "    if x_img_denoise is None:\n",
    "        x_embeddings, x_img_denoise = output_embeddings, output_x\n",
    "    else:\n",
    "        x_embeddings = np.concatenate((x_embeddings, output_embeddings), axis=0)\n",
    "        x_img_denoise = np.concatenate((x_img_denoise, output_x), axis=0)\n",
    "    \n",
    "x_img_denoise = np.array(x_img_denoise, dtype=np.float32)\n",
    "display(x_img_preprocess, x_img_noise)\n",
    "display(x_img_preprocess, x_img_denoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c0da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_img = np.array(x_img_denoise * 255.0, copy=True, dtype=np.uint8)\n",
    "# x_embeddings = scaler.inverse_transform(x_embeddings)\n",
    "# x_embeddings = np.array(x_embeddings, dtype=np.uint8)\n",
    "\n",
    "#print(x_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57441434",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_img_gray = np.array(x_img_denoise[:, :, :, 0] * 0.1140 + x_img_denoise[:, :, :, 1] * 0.5870 + x_img_denoise[:, :, :, 2] * 0.2989)\n",
    "x_img_mask = np.array([gray > gray.mean() for gray in x_img_gray], copy=True, dtype=np.uint8)\n",
    "x_img_mask = np.expand_dims(x_img_mask, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb8889",
   "metadata": {},
   "outputs": [],
   "source": [
    "images1 = np.array(x_img_gray[30:40], copy=True)\n",
    "images2 = np.array(x_img_mask[30:40], copy=True)\n",
    "n = 10\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i, (image1, image2) in enumerate(zip(images1, images2)):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(image1, cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(image2, cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860e4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_img = np.array(x_img * x_img_mask, copy=True, dtype=np.uint8)\n",
    "display(x_img_preprocess, x_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded10293",
   "metadata": {},
   "source": [
    "### Changing the contrast and brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2964c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookUpTable = np.empty((1,256), np.uint8)\n",
    "gamma = 1.3\n",
    "for i in range(256):\n",
    "    lookUpTable[0,i] = np.clip(pow(i / 255.0, gamma) * 255.0, 0, 255)\n",
    "\n",
    "def adjust_brightness(img, lookUpTable, alpha=1.3, beta=40):\n",
    "    new_image = np.zeros(img.shape, img.dtype)\n",
    "    \n",
    "    #for y in range(img.shape[0]):\n",
    "    #    for x in range(img.shape[1]):\n",
    "    #        for c in range(img.shape[2]):\n",
    "    #            new_image[y,x,c] = np.clip(alpha*img[y,x,c] + beta, 0, 255)\n",
    "\n",
    "    new_image = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
    "                \n",
    "    res = cv2.LUT(new_image, lookUpTable)\n",
    "                \n",
    "    return res\n",
    "\n",
    "# x_brightness = np.array([adjust_brightness(xi, lookUpTable) for xi in x_img], dtype=np.uint8)\n",
    "# print(x_brightness.shape)\n",
    "\n",
    "# x_preprocessed = np.array(x_brightness, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2141124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame = x_img[10]\n",
    "# result = x_brightness[10]\n",
    "\n",
    "# cv2.imshow('frame', frame)\n",
    "# cv2.imshow('result', result)\n",
    "\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5012f0",
   "metadata": {},
   "source": [
    "### Clustering filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_filter(img, n_clusters=5):\n",
    "    original_shape = img.shape\n",
    "    img = img.reshape(-1, 3)\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(img)\n",
    "\n",
    "    labels=kmeans.labels_\n",
    "    #print(labels)\n",
    "    labels=list(labels)\n",
    "\n",
    "    centroid=kmeans.cluster_centers_\n",
    "    #print(centroid)\n",
    "\n",
    "    percent=[]\n",
    "    for i in range(len(centroid)):\n",
    "      j=labels.count(i)\n",
    "      j=j/(len(labels))\n",
    "      percent.append(j)\n",
    "    #print(percent)\n",
    "\n",
    "    # bgr to rgb\n",
    "    #plt.pie(percent,colors=np.array(centroid[:, [2, 1, 0]]/255),labels=np.arange(len(centroid)))\n",
    "    #plt.show()\n",
    "\n",
    "    sorted_percent = sorted(percent)\n",
    "    remove_index = [percent_i in [sorted_percent[0], sorted_percent[1]] for percent_i in percent]\n",
    "    #print(remove_index)\n",
    "\n",
    "    result = np.array(img, copy=True)\n",
    "    for i, remove in enumerate(remove_index):\n",
    "        if remove:\n",
    "            result[labels==np.array(i)] = centroid[i]\n",
    "    result = result.reshape(original_shape)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# x_cluster = np.array([clustering_filter(xi) for xi in x_img], dtype=np.uint8)\n",
    "# print(x_cluster.shape)\n",
    "\n",
    "# x_preprocessed = np.array(x_cluster, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b1f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame = x_img[321]\n",
    "# result = x_cluster[321]\n",
    "\n",
    "# cv2.imshow('frame', frame)\n",
    "# cv2.imshow('result', result)\n",
    "\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9272ec",
   "metadata": {},
   "source": [
    "### HSV filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e4c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_sum = []\n",
    "\n",
    "def hsv_filter(img, init_value=100, end_value=0, average_value=20000, adaptive=False):\n",
    "    mask_value = 0\n",
    "    sv_value = init_value\n",
    "    \n",
    "    if adaptive:\n",
    "        while mask_value <= average_value and sv_value >= end_value:\n",
    "            # Threshold of blue in HSV space\n",
    "            lower_red = np.array([0,sv_value,sv_value])\n",
    "            upper_red = np.array([10,255,255])\n",
    "            hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "            # preparing the mask to overlay\n",
    "            mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "            mask_value = np.sum(mask/255)\n",
    "            sv_value -= 1\n",
    "    else:\n",
    "        lower_red = np.array([0,sv_value,sv_value])\n",
    "        upper_red = np.array([10,255,255])\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        # preparing the mask to overlay\n",
    "        mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "        mask_value = np.sum(mask/255)\n",
    "        \n",
    "    dummy_sum.append(mask_value)\n",
    "\n",
    "\n",
    "    # The black region in the mask has the value of 0,\n",
    "    # so when multiplied with original image removes all non-blue regions\n",
    "    result = cv2.bitwise_and(img, img, mask = mask)\n",
    "    \n",
    "    return result\n",
    "\n",
    "x_hsv = np.array([hsv_filter(xi) for xi in x_img], dtype=np.uint8)\n",
    "print(x_hsv.shape)\n",
    "\n",
    "print(np.mean(np.array(dummy_sum), axis=0))\n",
    "\n",
    "x_preprocessed = np.array(x_hsv, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b4bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame = x_img[60]\n",
    "# result = x_hsv[60]\n",
    "\n",
    "# cv2.imshow('frame', frame)\n",
    "# cv2.imshow('result', result)\n",
    "\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ac2cb",
   "metadata": {},
   "source": [
    "### Histrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c98dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blue, Green, Red and A (Transparency)\n",
    "def red_histogram(img):\n",
    "    return np.histogram(img[:, :, 2].flatten(), range(257))[0]\n",
    "\n",
    "x_hist = np.array([red_histogram(xi) for xi in x_preprocessed])\n",
    "print(x_hist.shape)\n",
    "\n",
    "x_final = x_hist\n",
    "y_final = y_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380753a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(256)[1:] - 0.5, x_hist[0][1:], width=1, edgecolor='none')\n",
    "plt.xlim([-0.5, 255.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e2db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savez_compressed('./x_hsv_hist', x=x_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca45762",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1458a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split by id\n",
    "x_train, y_train = x_final[[i > 80 for i in y_id]], y_final[[i > 80 for i in y_id]]\n",
    "x_test, y_test = x_final[[i <= 80 for i in y_id]], y_final[[i <= 80 for i in y_id]]\n",
    "\n",
    "#x_train, y_train = x_final[[i < 620 for i in y_id]], y_final[[i < 620 for i in y_id]]\n",
    "#x_test, y_test = x_final[[i >= 620 for i in y_id]], y_final[[i >= 620 for i in y_id]]\n",
    "\n",
    "if mode == \"classification\":\n",
    "    if binary:\n",
    "        print(\"train: (0)\", np.sum(y_train==0), \"(1)\", np.sum(y_train==1))\n",
    "        print(\"test: (0)\", np.sum(y_test==0), \"(1)\", np.sum(y_test==1))\n",
    "    else:\n",
    "        for i in range(len(threshold)+1):\n",
    "            print(i)\n",
    "            print(\"train:\", np.sum(y_train==i), \" test:\", np.sum(y_test==i))\n",
    "elif mode == \"regression\":\n",
    "    print(np.mean(y_train))\n",
    "    print(np.mean(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c85bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "# scaler = MinMaxScaler()\n",
    "# x_train = scaler.fit_transform(x_train)\n",
    "# x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be455b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"classification\":\n",
    "    if binary:\n",
    "        plt.plot(np.arange(256)[1:] - 0.5, x_train[y_train==0].mean(axis=0)[1:], label='non-anemia')\n",
    "        plt.fill_between(np.arange(256)[1:] - 0.5, x_train[y_train==0].mean(axis=0)[1:]-x_train[y_train==0].std(axis=0)[1:], x_train[y_train==0].mean(axis=0)[1:]+x_train[y_train==0].std(axis=0)[1:], alpha=0.4)\n",
    "        plt.plot(np.arange(256)[1:] - 0.5, x_train[y_train==1].mean(axis=0)[1:], label='anemia')\n",
    "        plt.fill_between(np.arange(256)[1:] - 0.5, x_train[y_train==1].mean(axis=0)[1:]-x_train[y_train==1].std(axis=0)[1:], x_train[y_train==1].mean(axis=0)[1:]+x_train[y_train==1].std(axis=0)[1:], alpha=0.4)\n",
    "    else:\n",
    "        for i in range(len(threshold)+1):\n",
    "            plt.plot(np.arange(256)[1:] - 0.5, x_train[y_train==i].mean(axis=0)[1:], label=threshold_name[i])\n",
    "            plt.fill_between(np.arange(256)[1:] - 0.5, x_train[y_train==i].mean(axis=0)[1:]-x_train[y_train==i].std(axis=0)[1:], x_train[y_train==i].mean(axis=0)[1:]+x_train[y_train==i].std(axis=0)[1:], alpha=0.4)\n",
    "elif mode == \"regression\":\n",
    "    plt.plot(np.arange(256)[1:] - 0.5, x_train[y_train>=12.5].mean(axis=0)[1:], label='non-anemia')\n",
    "    plt.fill_between(np.arange(256)[1:] - 0.5, x_train[y_train>=12.5].mean(axis=0)[1:]-x_train[y_train>=12.5].std(axis=0)[1:], x_train[y_train>=12.5].mean(axis=0)[1:]+x_train[y_train>=12.5].std(axis=0)[1:], alpha=0.4)\n",
    "    plt.plot(np.arange(256)[1:] - 0.5, x_train[y_train<12.5].mean(axis=0)[1:], label='anemia')\n",
    "    plt.fill_between(np.arange(256)[1:] - 0.5, x_train[y_train<12.5].mean(axis=0)[1:]-x_train[y_train<12.5].std(axis=0)[1:], x_train[y_train<12.5].mean(axis=0)[1:]+x_train[y_train<12.5].std(axis=0)[1:], alpha=0.4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8507a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"classification\":\n",
    "    if binary:\n",
    "        plt.plot(np.arange(256)[1:] - 0.5, x_test[y_test==0].mean(axis=0)[1:], label='non-anemia')\n",
    "        plt.fill_between(np.arange(256)[1:] - 0.5, x_test[y_test==0].mean(axis=0)[1:]-x_test[y_test==0].std(axis=0)[1:], x_test[y_test==0].mean(axis=0)[1:]+x_test[y_test==0].std(axis=0)[1:], alpha=0.4)\n",
    "        plt.plot(np.arange(256)[1:] - 0.5, x_test[y_test==1].mean(axis=0)[1:], label='anemia')\n",
    "        plt.fill_between(np.arange(256)[1:] - 0.5, x_test[y_test==1].mean(axis=0)[1:]-x_test[y_test==1].std(axis=0)[1:], x_test[y_test==1].mean(axis=0)[1:]+x_test[y_test==1].std(axis=0)[1:], alpha=0.4)\n",
    "    else:\n",
    "        for i in range(len(threshold)+1):\n",
    "            plt.plot(np.arange(256)[1:] - 0.5, x_test[y_test==i].mean(axis=0)[1:], label=threshold_name[i])\n",
    "            plt.fill_between(np.arange(256)[1:] - 0.5, x_test[y_test==i].mean(axis=0)[1:]-x_test[y_test==i].std(axis=0)[1:], x_test[y_test==i].mean(axis=0)[1:]+x_test[y_test==i].std(axis=0)[1:], alpha=0.4)\n",
    "elif mode == \"regression\":\n",
    "    plt.plot(np.arange(256)[1:] - 0.5, x_test[y_test>=12.5].mean(axis=0)[1:], label='non-anemia')\n",
    "    plt.fill_between(np.arange(256)[1:] - 0.5, x_test[y_test>=12.5].mean(axis=0)[1:]-x_test[y_test>=12.5].std(axis=0)[1:], x_test[y_test>=12.5].mean(axis=0)[1:]+x_test[y_test>=12.5].std(axis=0)[1:], alpha=0.4)\n",
    "    plt.plot(np.arange(256)[1:] - 0.5, x_test[y_test<12.5].mean(axis=0)[1:], label='anemia')\n",
    "    plt.fill_between(np.arange(256)[1:] - 0.5, x_test[y_test<12.5].mean(axis=0)[1:]-x_test[y_test<12.5].std(axis=0)[1:], x_test[y_test<12.5].mean(axis=0)[1:]+x_test[y_test<12.5].std(axis=0)[1:], alpha=0.4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724ceb83",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860501cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(y_true, y_hat, mode, binary):\n",
    "    if mode == \"classification\":\n",
    "        if binary:\n",
    "            average = \"binary\"\n",
    "            multi_class = \"raise\"\n",
    "            display_labels = [\"non-anemia\", \"anemia\"]\n",
    "        else:\n",
    "            average = \"macro\"\n",
    "            multi_class = \"ovo\"\n",
    "            display_labels = threshold_name\n",
    "            \n",
    "        print(\"accuracy: \", accuracy_score(y_true, np.argmax(y_hat, axis=1)))\n",
    "        print(\"precision: \", precision_score(y_true, np.argmax(y_hat, axis=1), average=average))\n",
    "        print(\"recall: \", recall_score(y_true, np.argmax(y_hat, axis=1), average=average))\n",
    "        if binary:\n",
    "            print(\"roc auc: \", roc_auc_score(y_true, y_hat[:, 1], multi_class=multi_class))\n",
    "        else:\n",
    "            print(\"roc auc: \", roc_auc_score(y_true, y_hat, multi_class=multi_class))\n",
    "        print(\"f1: \", f1_score(y_true, np.argmax(y_hat, axis=1), average=average))\n",
    "        print(\"cohen kappa score: \", cohen_kappa_score(y_true, np.argmax(y_hat, axis=1)))\n",
    "        y_hat = np.argmax(y_hat, axis=1)\n",
    "        cm = confusion_matrix(y_true, y_hat)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
    "        disp.plot()\n",
    "        plt.show()\n",
    "    elif mode == \"regression\":\n",
    "        print(\"mse: \", mean_squared_error(y_true, y_hat))\n",
    "        print(\"mae: \", mean_absolute_error(y_true, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027761d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"classification\":\n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"Linear Discriminant Analysis\")\n",
    "    print_results(y_test, clf.predict_proba(x_test), mode, binary)\n",
    "    \n",
    "    clf = LogisticRegression(random_state=0, max_iter=10000, solver='saga')\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"Logistic regression\")\n",
    "    print_results(y_test, clf.predict_proba(x_test), mode, binary)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=100, random_state=0)\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"Random forest\")\n",
    "    print_results(y_test, clf.predict_proba(x_test), mode, binary)\n",
    "\n",
    "    clf = SVC(random_state=0, C=5.0, probability=True)\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"SVM\")\n",
    "    print_results(y_test, clf.predict_proba(x_test), mode, binary)\n",
    "\n",
    "    clf = XGBClassifier(random_state=0)\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"XGBoost\")\n",
    "    print_results(y_test, clf.predict_proba(x_test), mode, binary)\n",
    "elif mode == \"regression\":\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"Linear regression\")\n",
    "    print_results(y_test, clf.predict(x_test), mode, binary)\n",
    "    \n",
    "    clf = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=0)\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"Random forest\")\n",
    "    print_results(y_test, clf.predict(x_test), mode, binary)\n",
    "    \n",
    "    clf = SVR(C=5.0)\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"SVM\")\n",
    "    print_results(y_test, clf.predict(x_test), mode, binary)\n",
    "    \n",
    "    clf = XGBRegressor(n_estimators=10, random_state=0)\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"XGBoost\")\n",
    "    print_results(y_test, clf.predict(x_test), mode, binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc5074a",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e08237",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"classification\":\n",
    "    x_img_train, y_img_train = x_img[[i > 80 for i in y_id]], y_img[[i > 80 for i in y_id]]\n",
    "    x_img_test, y_img_test = x_img[[i <= 80 for i in y_id]], y_img[[i <= 80 for i in y_id]]\n",
    "\n",
    "    predictions = np.argmax(clf.predict_proba(x_test), axis=1)\n",
    "    correct_index = predictions == y_test\n",
    "    for i, correct in enumerate(correct_index):\n",
    "        if correct:\n",
    "            correct_img = x_img_test[i]\n",
    "            correct_img = correct_img.astype(np.uint8)\n",
    "            cv2.putText(correct_img, str(y_test[i]), (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 1, cv2.LINE_AA)\n",
    "            cv2.imshow(\"correct\", correct_img)\n",
    "            cv2.waitKey(500)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a61c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"classification\":\n",
    "    wrong_index = predictions != y_test\n",
    "    for i, wrong in enumerate(wrong_index):\n",
    "        if wrong:\n",
    "            wrong_img = x_img_test[i]\n",
    "            wrong_img = wrong_img.astype(np.uint8)\n",
    "            cv2.putText(wrong_img, str(y_test[i]), (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 1, cv2.LINE_AA)\n",
    "            cv2.imshow(\"wrong\", wrong_img)\n",
    "            cv2.waitKey(500)\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
