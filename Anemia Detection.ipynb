{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0130fa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Conv2D, MaxPooling2D, UpSampling2D, Input, Rescaling, BatchNormalization, Reshape, Flatten\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e7909b",
   "metadata": {},
   "source": [
    "# Anemia prediction (Classification & Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ebbb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"classification\"\n",
    "#mode = \"regression\"\n",
    "binary = True\n",
    "threshold = [7.0, 10.0, 12.5]\n",
    "threshold_name = [\"severely anemic\", \"moderately anemic\", \"mildly anemic\", \"non-anemic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e209f854",
   "metadata": {},
   "source": [
    "## Load labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5de43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = os.path.join(\"D:\", \"OneDrive_1_5-26-2022\", \"PredictingAnemia_DATA_2022-06-05_0643.csv\")\n",
    "label = pd.read_csv(label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f3fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label[\"hgb\"] = pd.to_numeric(label[\"hgb\"], errors=\"coerce\")\n",
    "drop_index = np.where(pd.isnull(label[\"hgb\"]))\n",
    "print(\"drop (contains string or null): \", drop_index[0])\n",
    "label = label.drop(drop_index[0])\n",
    "print(\"mean:\", label[\"hgb\"].mean(), \"std:\", label[\"hgb\"].std())\n",
    "print(\"anemia mean: \", label[\"hgb\"][label[\"hgb\"] < 12.5].mean())\n",
    "print(\"non-anemia mean: \", label[\"hgb\"][label[\"hgb\"] >= 12.5].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9d53cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_class_label(label_i, threshold):\n",
    "    label = -1\n",
    "    for i, threshold_i in enumerate(threshold):\n",
    "        if label_i < threshold_i:\n",
    "            label = i\n",
    "            break\n",
    "    if label == -1:\n",
    "        label = len(threshold)\n",
    "    \n",
    "    #print(label, label_i)\n",
    "    \n",
    "    return label\n",
    "\n",
    "if mode == \"classification\":\n",
    "    if binary:\n",
    "        y = (label[\"hgb\"] < 12.5).astype(int)\n",
    "    else:\n",
    "        y = np.array([multi_class_label(label_i, threshold) for label_i in label[\"hgb\"]], dtype=np.uint8)\n",
    "        y = pd.Series(data=y, index=label[\"hgb\"].index)\n",
    "elif mode == \"regression\":\n",
    "    y = label[\"hgb\"]\n",
    "print(y.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc324d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_available = [] \n",
    "\n",
    "for folder in os.listdir(\"./detected eyes images\"):\n",
    "    if int(folder)-1 in y.index:\n",
    "        y_available.append(int(folder))\n",
    "        \n",
    "print(\"not available id: \")\n",
    "not_available_id = []\n",
    "for i in range(1, 693):\n",
    "    if i not in y_available:\n",
    "        not_available_id.append(i)\n",
    "print(not_available_id)\n",
    "print(\"num: \", len(not_available_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419b3320",
   "metadata": {},
   "source": [
    "## Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf44e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_img = []\n",
    "y_img = []\n",
    "y_id = []\n",
    "\n",
    "for id in y_available:\n",
    "    for image in os.listdir(os.path.join(\"./detected eyes images\", str(id))):\n",
    "        #print(id, image)\n",
    "        img = cv2.imread(os.path.join(\"./detected eyes images\", str(id), image))\n",
    "        #print(img.shape)\n",
    "        x_img.append(tf.image.resize(img, (224, 224)))\n",
    "        y_img.append(y[id-1])\n",
    "        y_id.append(id)\n",
    "        \n",
    "x_img = np.array(x_img, dtype=np.uint8)\n",
    "y_img = np.array(y_img)\n",
    "print(x_img.shape, y_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e272ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez(\"img_original_224_224_3.npz\", x_img=x_img, y_id=y_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b184c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_file = np.load(\"img_original_224_224_3.npz\")\n",
    "# x_img, y_id = load_file[\"x_img\"], load_file[\"y_id\"]\n",
    "# print(x_img.shape, y_id.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552220fd",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce77c67",
   "metadata": {},
   "source": [
    "### U-Net and autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dd5f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hsv_hist = np.load('./x_hsv_hist.npz')[\"x\"]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_hsv_hist = scaler.fit_transform(x_hsv_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d594fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(array):\n",
    "    \"\"\"\n",
    "    Normalizes the supplied array and reshapes it into the appropriate format.\n",
    "    \"\"\"\n",
    "\n",
    "    array = array.astype(\"float32\") / 255.0\n",
    "    array = np.reshape(array, (len(array), 224, 224, 3))\n",
    "    return array.astype(np.float32)\n",
    "\n",
    "def noise(array):\n",
    "    \"\"\"\n",
    "    Adds random noise to each image in the supplied array.\n",
    "    \"\"\"\n",
    "\n",
    "    noise_factor = 0.4\n",
    "    noisy_array = array + noise_factor * np.random.normal(\n",
    "        loc=0.0, scale=1.0, size=array.shape\n",
    "    )\n",
    "\n",
    "    return np.clip(noisy_array, 0.0, 1.0)\n",
    "\n",
    "def display(array1, array2):\n",
    "    \"\"\"\n",
    "    Displays ten random images from each one of the supplied arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    n = 10\n",
    "\n",
    "    indices = np.random.randint(len(array1), size=n)\n",
    "    \n",
    "    images1 = np.zeros_like(array1[indices, :])\n",
    "    images2 = np.zeros_like(array2[indices, :])\n",
    "    images1[:, :, :, 0] = array1[indices, :, :, 2]\n",
    "    images1[:, :, :, 1] = array1[indices, :, :, 1]\n",
    "    images1[:, :, :, 2] = array1[indices, :, :, 0]\n",
    "    images2[:, :, :, 0] = array2[indices, :, :, 2]\n",
    "    images2[:, :, :, 1] = array2[indices, :, :, 1]\n",
    "    images2[:, :, :, 2] = array2[indices, :, :, 0]\n",
    "\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i, (image1, image2) in enumerate(zip(images1, images2)):\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(image1)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(image2)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "x_img_preprocess = preprocess(x_img)\n",
    "x_img_noise = noise(x_img_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fbaa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vgg16(tf.keras.Model):\n",
    "    def __init__(self, pretrained = True):\n",
    "        super(Vgg16, self).__init__()\n",
    "        self.vggnet = tf.keras.applications.VGG16(include_top=False, weights=None)\n",
    "        \n",
    "    def call(self, x):\n",
    "        results = []\n",
    "        for ii,model in enumerate(self.vggnet.layers):\n",
    "            x = model(x)\n",
    "            if ii in [2,5,9,13,17]:\n",
    "                results.append(x) #(64,256,256),(128,128,128),(256,64,64),(512,32,32),(512,16,16)\n",
    "        return results\n",
    "\n",
    "vgg_model = Vgg16()\n",
    "vgg_model.build(input_shape=(None, 224, 224, 3))\n",
    "vgg_model.summary()\n",
    "\n",
    "class DeConv2d(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, stride, padding, dilation):\n",
    "        super().__init__()\n",
    "        self.up = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')\n",
    "        self.conv = tf.keras.layers.Conv2D(filters=out_channel, kernel_size=kernel_size, strides=stride, padding=padding, dilation_rate=dilation)\n",
    "    \n",
    "    def call(self, x):\n",
    "        output = self.up(x)\n",
    "        output = self.conv(output)\n",
    "        return output\n",
    "\n",
    "class UNet(tf.keras.Model):\n",
    "    def __init__(self, pretrained_net, n_class):\n",
    "        super().__init__()\n",
    "        self.n_class = n_class\n",
    "        self.pretrained_net = pretrained_net\n",
    "        #####################################\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.deconv1 = DeConv2d(512, 512, kernel_size=3, stride=1, padding=\"same\", dilation=1)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.deconv2 = DeConv2d(1024, 256, kernel_size=3, stride=1, padding=\"same\", dilation=1)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.deconv3 = DeConv2d(512, 128, kernel_size=3, stride=1, padding=\"same\", dilation=1)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.deconv4 = DeConv2d(256, 64, kernel_size=3, stride=1, padding=\"same\", dilation=1)\n",
    "        self.bn4 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.classifier = tf.keras.layers.Conv2D(n_class, kernel_size=1, activation=\"sigmoid\")\n",
    "        #####################################\n",
    "    \n",
    "    def call(self, x):\n",
    "        #####################################\n",
    "        pre_output = self.pretrained_net(x)\n",
    "        output = self.bn1(self.relu(self.deconv1(pre_output[4]))) #(512,32,32)\n",
    "        output = self.bn2(self.relu(self.deconv2(tf.concat([output, pre_output[3]], axis=-1)))) #(256,64,64)\n",
    "        output = self.bn3(self.relu(self.deconv3(tf.concat([output, pre_output[2]], axis=-1)))) #(128,128,128)\n",
    "        output = self.bn4(self.relu(self.deconv4(tf.concat([output, pre_output[1]], axis=-1)))) #(64,256,256)\n",
    "        output = self.classifier(tf.concat([output, pre_output[0]], axis=-1))\n",
    "        return output\n",
    "        #####################################\n",
    "        \n",
    "seg_model = UNet(pretrained_net=vgg_model, n_class=3)\n",
    "seg_model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "seg_model.build(input_shape=(None, 224, 224, 3))\n",
    "seg_model.summary()\n",
    "\n",
    "seg_model.fit(\n",
    "    x=x_img_preprocess,\n",
    "    y=x_img_preprocess,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "x_img_denoise = seg_model.predict(x_img_preprocess, batch_size=16)\n",
    "display(x_img_preprocess, x_img_noise)\n",
    "display(x_img_preprocess, x_img_denoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471eacb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.conv1 = Conv2D(256, (3, 3), activation='relu', padding='same')\n",
    "        self.max1 = MaxPooling2D((2, 2), padding='same')\n",
    "        self.batch1 = BatchNormalization()\n",
    "        self.conv2 = Conv2D(256, (3, 3), activation='relu', padding='same')\n",
    "        self.max2 = MaxPooling2D((2, 2), padding='same')\n",
    "        \n",
    "        # Embeddings\n",
    "        self.ave1 = GlobalAveragePooling2D()\n",
    "        self.dropout = Dropout(0.1)\n",
    "        \n",
    "        # Decoder\n",
    "        self.conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')\n",
    "        self.up3 = UpSampling2D((2, 2))\n",
    "        self.conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')\n",
    "        self.up4 = UpSampling2D((2, 2))\n",
    "        self.conv5 = Conv2D(3, (3, 3), activation='sigmoid', padding='same')\n",
    "        \n",
    "    def call(self, x, training):\n",
    "        x_img, x_hist = x\n",
    "        x = self.conv1(x_img)\n",
    "        x = self.max1(x)\n",
    "        x = self.batch1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.max2(x)\n",
    "        \n",
    "        #embeddings = self.dropout(x_hist, training=training) + self.ave1(x)\n",
    "        embeddings = x_hist + self.ave1(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.up3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.up4(x)\n",
    "        x = self.conv5(x)\n",
    "        return embeddings, x\n",
    "\n",
    "autoencoder = Autoencoder()\n",
    "autoencoder.build(input_shape=[(None, 224, 224, 3), (None, 256)])\n",
    "autoencoder.summary()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "bce_loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "bce_loss_metric = tf.keras.metrics.Mean()\n",
    "mse_loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices({\"img\": x_img_preprocess, \"hist\": x_hsv_hist})\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(16)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    #print(\"Start of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "    bce_loss_metric.reset_state()\n",
    "    mse_loss_metric.reset_state()\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        x_batch_img, x_batch_hsv_hist = x_batch_train[\"img\"], x_batch_train[\"hist\"]\n",
    "        with tf.GradientTape() as tape:\n",
    "            embeddings, reconstructed = autoencoder([x_batch_img, x_batch_hsv_hist], training=True)\n",
    "            # Compute reconstruction loss\n",
    "            loss = bce_loss_fn(x_batch_img, reconstructed) + mse_loss_fn(x_batch_hsv_hist, embeddings) * 10\n",
    "\n",
    "        grads = tape.gradient(loss, autoencoder.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
    "\n",
    "        bce_loss_metric.update_state(bce_loss_fn(x_batch_img, reconstructed))\n",
    "        mse_loss_metric.update_state(mse_loss_fn(x_batch_hsv_hist, embeddings) * 10)\n",
    "        \n",
    "    print(\"epoch %d: bce mean loss = %.4f, mse mean loss = %.4f, elapsed time: %ds\" % (epoch, bce_loss_metric.result(), mse_loss_metric.result(), time.time()-start_time))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices({\"img\": x_img_preprocess, \"hist\": x_hsv_hist})\n",
    "test_dataset = test_dataset.batch(16)\n",
    "    \n",
    "x_img_denoise = None\n",
    "x_embeddings = None\n",
    "for x_batch_test in test_dataset:\n",
    "    x_batch_img, x_batch_hsv_hist = x_batch_test[\"img\"], x_batch_test[\"hist\"]\n",
    "    output_embeddings, output_x = autoencoder([x_batch_img, x_batch_hsv_hist], training=False)\n",
    "    if x_img_denoise is None:\n",
    "        x_embeddings, x_img_denoise = output_embeddings, output_x\n",
    "    else:\n",
    "        x_embeddings = np.concatenate((x_embeddings, output_embeddings), axis=0)\n",
    "        x_img_denoise = np.concatenate((x_img_denoise, output_x), axis=0)\n",
    "    \n",
    "x_img_denoise = np.array(x_img_denoise, dtype=np.float32)\n",
    "display(x_img_preprocess, x_img_noise)\n",
    "display(x_img_preprocess, x_img_denoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c0da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_img = np.array(x_img_denoise * 255.0, copy=True, dtype=np.uint8)\n",
    "# x_embeddings = scaler.inverse_transform(x_embeddings)\n",
    "# x_embeddings = np.array(x_embeddings, dtype=np.uint8)\n",
    "\n",
    "#print(x_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca62563",
   "metadata": {},
   "source": [
    "### Mask (Segmentation output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b49f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_mask = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2605b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_mask = True\n",
    "\n",
    "mask_data = np.load(\"./mask autoencoder/mask_autoencoder_original_224_224_3.npz\")\n",
    "x_mask_raw = np.expand_dims(mask_data['x_mask'], axis=-1)\n",
    "x_mask = []\n",
    "for img in x_mask_raw:\n",
    "    x_mask.append(tf.image.resize(img, (224, 224)))\n",
    "x_mask = np.array(x_mask, dtype=np.uint8)\n",
    "print(mask_data[\"y_id\"])\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i, (x_img_i, x_mask_i) in enumerate(zip(x_img[1280:1290], x_mask[1280:1290])):\n",
    "    x_img_i_rgb = np.zeros_like(x_img_i)\n",
    "    x_img_i_rgb[:, :, 0] = x_img_i[:, :, 2]\n",
    "    x_img_i_rgb[:, :, 1] = x_img_i[:, :, 1]\n",
    "    x_img_i_rgb[:, :, 2] = x_img_i[:, :, 0]\n",
    "    ax = plt.subplot(2, 10, i + 1)\n",
    "    plt.imshow(x_img_i_rgb)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    ax = plt.subplot(2, 10, i + 1 + 10)\n",
    "    plt.imshow(x_mask_i, cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4c2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_img_masked = x_img * (x_mask / 255.0)\n",
    "x_img_masked = np.array(x_img_masked, copy=True, dtype=np.uint8)\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i, x_img_i in enumerate(x_img_masked[1280:1290]):\n",
    "    x_img_i_rgb = np.zeros_like(x_img_i)\n",
    "    x_img_i_rgb[:, :, 0] = x_img_i[:, :, 2]\n",
    "    x_img_i_rgb[:, :, 1] = x_img_i[:, :, 1]\n",
    "    x_img_i_rgb[:, :, 2] = x_img_i[:, :, 0]\n",
    "    ax = plt.subplot(1, 10, i + 1)\n",
    "    plt.imshow(x_img_i_rgb)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(y_img[1280:1290])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa640d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster\n",
    "# Find centroids\n",
    "\n",
    "def clustering_slice(original_img, mask_img, n_clusters=2):\n",
    "    original_shape = original_img.shape\n",
    "    mask_img = mask_img.reshape(-1, 3)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, max_iter=500)\n",
    "    kmeans.fit(mask_img)\n",
    "\n",
    "    labels = kmeans.labels_\n",
    "    labels = labels.reshape(original_shape[:2])\n",
    "    left, right, top, bottom = label.shape[0], 0, labels.shape[1], 0\n",
    "    for i in range(labels.shape[0]):\n",
    "        for j in range(labels.shape[1]):\n",
    "            if labels[i][j] == 1:\n",
    "                if i < left:\n",
    "                    left = i\n",
    "                if i > right:\n",
    "                    right = i\n",
    "                if j < top:\n",
    "                    top = j\n",
    "                if j > bottom:\n",
    "                    bottom = j\n",
    "    #print(left, right, top, bottom)\n",
    "\n",
    "    inside = original_img[left:right, top:bottom, :]\n",
    "    outside = np.array(original_img)\n",
    "    outside[left:right, top:bottom, :] = 0\n",
    "    \n",
    "    # TODO: expansion / shrinkage, fixed dimension\n",
    "    \n",
    "    \n",
    "    return inside, outside\n",
    "\n",
    "x_inside = []\n",
    "x_outside = []\n",
    "for i in range(len(x_img)):\n",
    "    #print(i)\n",
    "    inside, outside = clustering_slice(x_img[i], x_img_masked[i])\n",
    "    x_inside.append(inside)\n",
    "    x_outside.append(outside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7bb0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_img_valid = []\n",
    "x_mask_valid = []\n",
    "x_img_masked_valid = []\n",
    "x_inside_valid = []\n",
    "x_outside_valid = []\n",
    "y_img_valid = []\n",
    "y_id_valid = []\n",
    "\n",
    "for i, img in enumerate(x_inside):\n",
    "    #print(i.shape)\n",
    "    if img.shape[0] != 0:\n",
    "        x_inside_valid.append(img)\n",
    "        x_outside_valid.append(x_outside[i])\n",
    "        y_img_valid.append(y_img[i])\n",
    "        x_img_valid.append(x_img[i])\n",
    "        x_mask_valid.append(x_mask[i])\n",
    "        x_img_masked_valid.append(x_img_masked[i])\n",
    "        y_id_valid.append(y_id[i])\n",
    "        \n",
    "y_img_valid = np.array(y_img_valid)\n",
    "x_img_valid = np.array(x_img_valid)\n",
    "x_mask_valid = np.array(x_mask_valid)\n",
    "x_outside_valid = np.array(x_outside_valid)\n",
    "\n",
    "print(\"number of images with all black: \", len(x_inside)-len(x_inside_valid))\n",
    "\n",
    "# remove all black images and its corresponding labels\n",
    "x_inside = x_inside_valid\n",
    "x_outside = x_outside_valid\n",
    "y_img = y_img_valid\n",
    "x_img = x_img_valid\n",
    "x_mask = x_mask_valid\n",
    "x_img_masked = x_img_masked_valid\n",
    "y_id = y_id_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb667fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = 1200\n",
    "# cv2.imshow('img', x_img[index])\n",
    "# cv2.imshow('mask', x_mask[index])\n",
    "# cv2.imshow('img_mask', x_img_masked[index])\n",
    "# cv2.imshow('inside', x_inside[index])\n",
    "# cv2.imshow('outside', x_outside[index])\n",
    "# print(x_inside[index].shape)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b46b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_shape = [0, 0]\n",
    "width = []\n",
    "height = []\n",
    "area = []\n",
    "for i in x_inside:\n",
    "    mean_shape[0] += i.shape[0]\n",
    "    mean_shape[1] += i.shape[1]\n",
    "    width.append(int(i.shape[0]))\n",
    "    height.append(int(i.shape[1]))\n",
    "    area.append(int(i.shape[0]*i.shape[1]))\n",
    "\n",
    "mean_shape[0] /= len(x_inside)\n",
    "mean_shape[1] /= len(x_inside)\n",
    "\n",
    "print(mean_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1601ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "width_hist = np.histogram(width, range(225))[0]\n",
    "plt.bar(range(224), width_hist, width=1, edgecolor='none')\n",
    "plt.title(\"width histogram\")\n",
    "plt.xlabel(\"width\")\n",
    "plt.ylabel(\"number of images\")\n",
    "plt.show()\n",
    "print(np.mean(width), np.std(width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982f34c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "height_hist = np.histogram(height, range(225))[0]\n",
    "plt.bar(range(224), height_hist, width=1, edgecolor='none')\n",
    "plt.title(\"height histogram\")\n",
    "plt.xlabel(\"height\")\n",
    "plt.ylabel(\"number of images\")\n",
    "plt.show()\n",
    "print(np.mean(height), np.std(height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be441fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_hist = np.histogram(area, len(np.bincount(area))//100)[0]\n",
    "plt.bar(np.arange(len(np.bincount(area))//100), area_hist, width=1, edgecolor='none')\n",
    "plt.title(\"area histogram\")\n",
    "plt.xlabel(\"area (width * height / 100)\")\n",
    "plt.ylabel(\"number of images\")\n",
    "plt.show()\n",
    "print(np.mean(area), np.std(area))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3376131",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_img = x_inside"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded10293",
   "metadata": {},
   "source": [
    "### Changing the contrast and brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2964c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookUpTable = np.empty((1,256), np.uint8)\n",
    "gamma = 1.3\n",
    "for i in range(256):\n",
    "    lookUpTable[0,i] = np.clip(pow(i / 255.0, gamma) * 255.0, 0, 255)\n",
    "\n",
    "def adjust_brightness(img, lookUpTable, alpha=1.3, beta=40):\n",
    "    new_image = np.zeros(img.shape, img.dtype)\n",
    "    \n",
    "    #for y in range(img.shape[0]):\n",
    "    #    for x in range(img.shape[1]):\n",
    "    #        for c in range(img.shape[2]):\n",
    "    #            new_image[y,x,c] = np.clip(alpha*img[y,x,c] + beta, 0, 255)\n",
    "\n",
    "    new_image = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
    "                \n",
    "    res = cv2.LUT(new_image, lookUpTable)\n",
    "                \n",
    "    return res\n",
    "\n",
    "# x_brightness = np.array([adjust_brightness(xi, lookUpTable) for xi in x_img], dtype=np.uint8)\n",
    "# print(x_brightness.shape)\n",
    "\n",
    "# x_preprocessed = np.array(x_brightness, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2141124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame = x_img[10]\n",
    "# result = x_brightness[10]\n",
    "\n",
    "# cv2.imshow('frame', frame)\n",
    "# cv2.imshow('result', result)\n",
    "\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5012f0",
   "metadata": {},
   "source": [
    "### Clustering filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_filter(img, n_clusters=5):\n",
    "    original_shape = img.shape\n",
    "    img = img.reshape(-1, 3)\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(img)\n",
    "\n",
    "    labels=kmeans.labels_\n",
    "    #print(labels)\n",
    "    labels=list(labels)\n",
    "\n",
    "    centroid=kmeans.cluster_centers_\n",
    "    #print(centroid)\n",
    "\n",
    "    percent=[]\n",
    "    for i in range(len(centroid)):\n",
    "      j=labels.count(i)\n",
    "      j=j/(len(labels))\n",
    "      percent.append(j)\n",
    "    #print(percent)\n",
    "\n",
    "    # bgr to rgb\n",
    "    #plt.pie(percent,colors=np.array(centroid[:, [2, 1, 0]]/255),labels=np.arange(len(centroid)))\n",
    "    #plt.show()\n",
    "\n",
    "    sorted_percent = sorted(percent)\n",
    "    remove_index = [percent_i in [sorted_percent[0], sorted_percent[1]] for percent_i in percent]\n",
    "    #print(remove_index)\n",
    "\n",
    "    result = np.array(img, copy=True)\n",
    "    for i, remove in enumerate(remove_index):\n",
    "        if remove:\n",
    "            result[labels==np.array(i)] = centroid[i]\n",
    "    result = result.reshape(original_shape)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# x_cluster = np.array([clustering_filter(xi) for xi in x_img], dtype=np.uint8)\n",
    "# print(x_cluster.shape)\n",
    "\n",
    "# x_preprocessed = np.array(x_cluster, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b1f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame = x_img[321]\n",
    "# result = x_cluster[321]\n",
    "\n",
    "# cv2.imshow('frame', frame)\n",
    "# cv2.imshow('result', result)\n",
    "\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9272ec",
   "metadata": {},
   "source": [
    "### HSV filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e4c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_sum = []\n",
    "\n",
    "def hsv_filter(img, init_value=100, end_value=0, average_value=20000, adaptive=False):\n",
    "    mask_value = 0\n",
    "    sv_value = init_value\n",
    "    \n",
    "    if adaptive:\n",
    "        while mask_value <= average_value and sv_value >= end_value:\n",
    "            # Threshold of blue in HSV space\n",
    "            lower_red = np.array([0,sv_value,sv_value])\n",
    "            upper_red = np.array([10,255,255])\n",
    "            hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "            # preparing the mask to overlay\n",
    "            mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "            mask_value = np.sum(mask/255)\n",
    "            sv_value -= 1\n",
    "    else:\n",
    "        lower_red = np.array([0,sv_value,sv_value])\n",
    "        upper_red = np.array([10,255,255])\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        # preparing the mask to overlay\n",
    "        mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "        mask_value = np.sum(mask/255)\n",
    "        \n",
    "    dummy_sum.append(mask_value)\n",
    "\n",
    "\n",
    "    # The black region in the mask has the value of 0,\n",
    "    # so when multiplied with original image removes all non-blue regions\n",
    "    result = cv2.bitwise_and(img, img, mask = mask)\n",
    "    \n",
    "    return result\n",
    "\n",
    "if not use_mask:\n",
    "    x_hsv = np.array([hsv_filter(xi, 120) for xi in x_img], dtype=np.uint8)\n",
    "    print(x_hsv.shape)\n",
    "    x_preprocessed = np.array(x_hsv, copy=True)\n",
    "else:\n",
    "    x_hsv = [hsv_filter(xi, 120) for xi in x_img]\n",
    "    x_preprocessed = x_hsv\n",
    "\n",
    "print(np.mean(np.array(dummy_sum), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b4bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame = x_img[60]\n",
    "# result = x_hsv[60]\n",
    "\n",
    "# cv2.imshow('frame', frame)\n",
    "# cv2.imshow('result', result)\n",
    "\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ac2cb",
   "metadata": {},
   "source": [
    "### Histrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c98dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blue, Green, Red and A (Transparency)\n",
    "def red_histogram(img):\n",
    "    return np.histogram(img[:, :, 2].flatten(), range(257))[0]\n",
    "\n",
    "x_hist = np.array([red_histogram(xi) for xi in x_preprocessed])\n",
    "print(x_hist.shape)\n",
    "\n",
    "x_final = x_hist\n",
    "y_final = y_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380753a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(256)[1:] - 0.5, x_hist[0][1:], width=1, edgecolor='none')\n",
    "plt.xlim([-0.5, 255.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e2db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savez_compressed('./x_hsv_hist', x=x_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb7b370",
   "metadata": {},
   "source": [
    "## Mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6406a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_feature(img, mask=False):\n",
    "    flatten_img = img.reshape(-1, 3)\n",
    "    if mask:\n",
    "        index = []\n",
    "        for i, pixel in enumerate(flatten_img):\n",
    "            if not (pixel == [0, 0, 0]).all():\n",
    "                index.append(i)\n",
    "        if len(index) == 0:\n",
    "            gbr_mean = np.array([0, 0, 0], dtype=np.float32)\n",
    "        else:\n",
    "            gbr_mean = np.mean(flatten_img[index], axis=0)\n",
    "    else:\n",
    "        gbr_mean = np.mean(flatten_img, axis=0)\n",
    "\n",
    "    hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    flatten_hsv_img = hsv_img.reshape(-1, 3)\n",
    "    if mask:\n",
    "        if len(index) == 0:\n",
    "            hsv_mean = np.array([0, 0, 0], dtype=np.float32)\n",
    "        else:\n",
    "            hsv_mean = np.mean(flatten_hsv_img[index], axis=0)\n",
    "    else:\n",
    "        hsv_mean = np.mean(flatten_hsv_img, axis=0)\n",
    "    \n",
    "    return np.concatenate((gbr_mean, hsv_mean), axis=0)\n",
    "\n",
    "x_mean = np.array([mean_feature(xi) for xi in x_preprocessed])\n",
    "print(x_mean.shape)\n",
    "\n",
    "x_final = x_mean\n",
    "y_final = y_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5826a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_hsv = np.array([hsv_filter(xi, 120) for xi in x_img], dtype=np.uint8)\n",
    "# print(x_hsv.shape)\n",
    "# print(np.mean(np.array(dummy_sum), axis=0))\n",
    "# x_preprocessed = np.array(x_hsv, copy=True)\n",
    "# x_mean = np.array([mean_feature(xi) for xi in x_preprocessed])\n",
    "# print(x_mean.shape)\n",
    "# x_mean_list = np.array(x_mean)\n",
    "\n",
    "# # x_hsv = np.array([hsv_filter(xi, 80) for xi in x_img], dtype=np.uint8)\n",
    "# # print(x_hsv.shape)\n",
    "# # print(np.mean(np.array(dummy_sum), axis=0))\n",
    "# # x_preprocessed = np.array(x_hsv, copy=True)\n",
    "# # x_mean = np.array([mean_feature(xi) for xi in x_preprocessed])\n",
    "# # print(x_mean.shape)\n",
    "\n",
    "# # x_mean_list = np.concatenate((x_mean_list, x_mean), axis=1)\n",
    "# # print(x_mean_list.shape)\n",
    "\n",
    "\n",
    "# x_final = x_mean_list\n",
    "# y_final = y_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca45762",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1458a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split by id\n",
    "x_train, y_train = x_final[[i > 80 for i in y_id]], y_final[[i > 80 for i in y_id]]\n",
    "x_test, y_test = x_final[[i <= 80 for i in y_id]], y_final[[i <= 80 for i in y_id]]\n",
    "\n",
    "#x_train, y_train = x_final[[i < 620 for i in y_id]], y_final[[i < 620 for i in y_id]]\n",
    "#x_test, y_test = x_final[[i >= 620 for i in y_id]], y_final[[i >= 620 for i in y_id]]\n",
    "\n",
    "if mode == \"classification\":\n",
    "    if binary:\n",
    "        print(\"train: (0)\", np.sum(y_train==0), \"(1)\", np.sum(y_train==1))\n",
    "        print(\"test: (0)\", np.sum(y_test==0), \"(1)\", np.sum(y_test==1))\n",
    "    else:\n",
    "        for i in range(len(threshold)+1):\n",
    "            print(i)\n",
    "            print(\"train:\", np.sum(y_train==i), \" test:\", np.sum(y_test==i))\n",
    "elif mode == \"regression\":\n",
    "    print(np.mean(y_train))\n",
    "    print(np.mean(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c85bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "# scaler = MinMaxScaler()\n",
    "# x_train = scaler.fit_transform(x_train)\n",
    "# x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be455b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"classification\":\n",
    "    if binary:\n",
    "        plt.plot(np.arange(256)[1:] - 0.5, x_train[y_train==0].mean(axis=0)[1:], label='non-anemia')\n",
    "        plt.fill_between(np.arange(256)[1:] - 0.5, x_train[y_train==0].mean(axis=0)[1:]-x_train[y_train==0].std(axis=0)[1:], x_train[y_train==0].mean(axis=0)[1:]+x_train[y_train==0].std(axis=0)[1:], alpha=0.4)\n",
    "        plt.plot(np.arange(256)[1:] - 0.5, x_train[y_train==1].mean(axis=0)[1:], label='anemia')\n",
    "        plt.fill_between(np.arange(256)[1:] - 0.5, x_train[y_train==1].mean(axis=0)[1:]-x_train[y_train==1].std(axis=0)[1:], x_train[y_train==1].mean(axis=0)[1:]+x_train[y_train==1].std(axis=0)[1:], alpha=0.4)\n",
    "    else:\n",
    "        for i in range(len(threshold)+1):\n",
    "            plt.plot(np.arange(256)[1:] - 0.5, x_train[y_train==i].mean(axis=0)[1:], label=threshold_name[i])\n",
    "            plt.fill_between(np.arange(256)[1:] - 0.5, x_train[y_train==i].mean(axis=0)[1:]-x_train[y_train==i].std(axis=0)[1:], x_train[y_train==i].mean(axis=0)[1:]+x_train[y_train==i].std(axis=0)[1:], alpha=0.4)\n",
    "elif mode == \"regression\":\n",
    "    plt.plot(np.arange(256)[1:] - 0.5, x_train[y_train>=12.5].mean(axis=0)[1:], label='non-anemia')\n",
    "    plt.fill_between(np.arange(256)[1:] - 0.5, x_train[y_train>=12.5].mean(axis=0)[1:]-x_train[y_train>=12.5].std(axis=0)[1:], x_train[y_train>=12.5].mean(axis=0)[1:]+x_train[y_train>=12.5].std(axis=0)[1:], alpha=0.4)\n",
    "    plt.plot(np.arange(256)[1:] - 0.5, x_train[y_train<12.5].mean(axis=0)[1:], label='anemia')\n",
    "    plt.fill_between(np.arange(256)[1:] - 0.5, x_train[y_train<12.5].mean(axis=0)[1:]-x_train[y_train<12.5].std(axis=0)[1:], x_train[y_train<12.5].mean(axis=0)[1:]+x_train[y_train<12.5].std(axis=0)[1:], alpha=0.4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8507a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"classification\":\n",
    "    if binary:\n",
    "        plt.plot(np.arange(256)[1:] - 0.5, x_test[y_test==0].mean(axis=0)[1:], label='non-anemia')\n",
    "        plt.fill_between(np.arange(256)[1:] - 0.5, x_test[y_test==0].mean(axis=0)[1:]-x_test[y_test==0].std(axis=0)[1:], x_test[y_test==0].mean(axis=0)[1:]+x_test[y_test==0].std(axis=0)[1:], alpha=0.4)\n",
    "        plt.plot(np.arange(256)[1:] - 0.5, x_test[y_test==1].mean(axis=0)[1:], label='anemia')\n",
    "        plt.fill_between(np.arange(256)[1:] - 0.5, x_test[y_test==1].mean(axis=0)[1:]-x_test[y_test==1].std(axis=0)[1:], x_test[y_test==1].mean(axis=0)[1:]+x_test[y_test==1].std(axis=0)[1:], alpha=0.4)\n",
    "    else:\n",
    "        for i in range(len(threshold)+1):\n",
    "            plt.plot(np.arange(256)[1:] - 0.5, x_test[y_test==i].mean(axis=0)[1:], label=threshold_name[i])\n",
    "            plt.fill_between(np.arange(256)[1:] - 0.5, x_test[y_test==i].mean(axis=0)[1:]-x_test[y_test==i].std(axis=0)[1:], x_test[y_test==i].mean(axis=0)[1:]+x_test[y_test==i].std(axis=0)[1:], alpha=0.4)\n",
    "elif mode == \"regression\":\n",
    "    plt.plot(np.arange(256)[1:] - 0.5, x_test[y_test>=12.5].mean(axis=0)[1:], label='non-anemia')\n",
    "    plt.fill_between(np.arange(256)[1:] - 0.5, x_test[y_test>=12.5].mean(axis=0)[1:]-x_test[y_test>=12.5].std(axis=0)[1:], x_test[y_test>=12.5].mean(axis=0)[1:]+x_test[y_test>=12.5].std(axis=0)[1:], alpha=0.4)\n",
    "    plt.plot(np.arange(256)[1:] - 0.5, x_test[y_test<12.5].mean(axis=0)[1:], label='anemia')\n",
    "    plt.fill_between(np.arange(256)[1:] - 0.5, x_test[y_test<12.5].mean(axis=0)[1:]-x_test[y_test<12.5].std(axis=0)[1:], x_test[y_test<12.5].mean(axis=0)[1:]+x_test[y_test<12.5].std(axis=0)[1:], alpha=0.4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97112b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"classification\":\n",
    "    if binary:\n",
    "        plt.plot(np.arange(x_train.shape[1]), x_train[y_train==0].mean(axis=0), label='non-anemia')\n",
    "        plt.fill_between(np.arange(x_train.shape[1]), x_train[y_train==0].mean(axis=0)-x_train[y_train==0].std(axis=0), x_train[y_train==0].mean(axis=0)+x_train[y_train==0].std(axis=0), alpha=0.4)\n",
    "        plt.plot(np.arange(x_train.shape[1]), x_train[y_train==1].mean(axis=0), label='anemia')\n",
    "        plt.fill_between(np.arange(x_train.shape[1]), x_train[y_train==1].mean(axis=0)-x_train[y_train==1].std(axis=0), x_train[y_train==1].mean(axis=0)+x_train[y_train==1].std(axis=0), alpha=0.4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123d84d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"classification\":\n",
    "    if binary:\n",
    "        plt.plot(np.arange(x_test.shape[1]), x_test[y_test==0].mean(axis=0), label='non-anemia')\n",
    "        plt.fill_between(np.arange(x_test.shape[1]), x_test[y_test==0].mean(axis=0)-x_test[y_test==0].std(axis=0), x_test[y_test==0].mean(axis=0)+x_test[y_test==0].std(axis=0), alpha=0.4)\n",
    "        plt.plot(np.arange(x_test.shape[1]), x_test[y_test==1].mean(axis=0), label='anemia')\n",
    "        plt.fill_between(np.arange(x_test.shape[1]), x_test[y_test==1].mean(axis=0)-x_test[y_test==1].std(axis=0), x_test[y_test==1].mean(axis=0)+x_test[y_test==1].std(axis=0), alpha=0.4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724ceb83",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860501cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(y_true, y_hat, mode, binary, debug):\n",
    "    results = {}\n",
    "    if mode == \"classification\":\n",
    "        if binary:\n",
    "            average = \"binary\"\n",
    "            multi_class = \"raise\"\n",
    "            display_labels = [\"non-anemia\", \"anemia\"]\n",
    "        else:\n",
    "            average = \"macro\"\n",
    "            multi_class = \"ovo\"\n",
    "            display_labels = threshold_name    \n",
    "        results[\"accuracy\"] = accuracy_score(y_true, np.argmax(y_hat, axis=1))\n",
    "        results[\"precision\"] = precision_score(y_true, np.argmax(y_hat, axis=1), average=average)\n",
    "        results[\"recall\"] = recall_score(y_true, np.argmax(y_hat, axis=1), average=average)\n",
    "        if binary:\n",
    "            results[\"roc_auc\"] = roc_auc_score(y_true, y_hat[:, 1], multi_class=multi_class)\n",
    "        else:\n",
    "            results[\"roc_auc\"] = roc_auc_score(y_true, y_hat, multi_class=multi_class)\n",
    "        results[\"f1\"] = f1_score(y_true, np.argmax(y_hat, axis=1), average=average)\n",
    "        results[\"cohen_kappa\"] = cohen_kappa_score(y_true, np.argmax(y_hat, axis=1))\n",
    "        y_hat = np.argmax(y_hat, axis=1)\n",
    "        cm = confusion_matrix(y_true, y_hat)\n",
    "        if debug:\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
    "            disp.plot()\n",
    "            plt.show()\n",
    "    elif mode == \"regression\":\n",
    "        results[\"mse\"] = mean_squared_error(y_true, y_hat)\n",
    "        results[\"mae\"] = mean_absolute_error(y_true, y_hat)\n",
    "        \n",
    "    if debug:\n",
    "        for key, value in results.items():\n",
    "            print(key, \": \", value)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01926e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_round(x_train, y_train, x_test, y_test, debug=True):\n",
    "    prediction_csv = {}\n",
    "    prediction_csv[\"ground truth\"] = y_test\n",
    "    prediction_results = {}\n",
    "\n",
    "    if mode == \"classification\":\n",
    "        clf = LinearDiscriminantAnalysis()\n",
    "        clf.fit(x_train, y_train)\n",
    "        if debug:\n",
    "            print(\"Linear Discriminant Analysis\")\n",
    "        prediction_results[\"Linear Discriminant Analysis\"] = print_results(y_test, clf.predict_proba(x_test), mode, binary, debug)\n",
    "        prediction_csv[\"Linear Discriminant Analysis\"] = clf.predict_proba(x_test)[:, 1] >= 0.5\n",
    "\n",
    "        clf = LogisticRegression(random_state=0, max_iter=10000, solver='saga')\n",
    "        clf.fit(x_train, y_train)\n",
    "        if debug:\n",
    "            print(\"Logistic regression\")\n",
    "        prediction_results[\"Logistic regression\"] = print_results(y_test, clf.predict_proba(x_test), mode, binary, debug)\n",
    "        prediction_csv[\"Logistic regression\"] = clf.predict_proba(x_test)[:, 1] >= 0.5\n",
    "\n",
    "        clf = RandomForestClassifier(n_estimators=100, max_depth=100, random_state=0)\n",
    "        clf.fit(x_train, y_train)\n",
    "        if debug:\n",
    "            print(\"Random forest\")\n",
    "        prediction_results[\"Random forest\"] = print_results(y_test, clf.predict_proba(x_test), mode, binary, debug)\n",
    "        prediction_csv[\"Random forest\"] = clf.predict_proba(x_test)[:, 1] >= 0.5\n",
    "\n",
    "        clf = SVC(random_state=0, C=5.0, probability=True)\n",
    "        clf.fit(x_train, y_train)\n",
    "        if debug:\n",
    "            print(\"SVM\")\n",
    "        prediction_results[\"SVM\"] = print_results(y_test, clf.predict_proba(x_test), mode, binary, debug)\n",
    "        prediction_csv[\"SVM\"] = clf.predict_proba(x_test)[:, 1] >= 0.5\n",
    "\n",
    "        clf = XGBClassifier(random_state=0)\n",
    "        clf.fit(x_train, y_train)\n",
    "        if debug:\n",
    "            print(\"XGBoost\")\n",
    "        prediction_results[\"XGBoost\"] = print_results(y_test, clf.predict_proba(x_test), mode, binary, debug)\n",
    "        prediction_csv[\"XGBoost\"] = clf.predict_proba(x_test)[:, 1] >= 0.5\n",
    "    elif mode == \"regression\":\n",
    "        clf = LinearRegression()\n",
    "        clf.fit(x_train, y_train)\n",
    "        if debug:\n",
    "            print(\"Linear regression\")\n",
    "        prediction_results[\"Linear regression\"] = print_results(y_test, clf.predict(x_test), mode, binary, debug)\n",
    "\n",
    "        clf = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=0)\n",
    "        clf.fit(x_train, y_train)\n",
    "        if debug:\n",
    "            print(\"Random forest\")\n",
    "        prediction_results[\"Random forest\"] = print_results(y_test, clf.predict(x_test), mode, binary, debug)\n",
    "\n",
    "        clf = SVR(C=5.0)\n",
    "        clf.fit(x_train, y_train)\n",
    "        if debug:\n",
    "            print(\"SVM\")\n",
    "        prediction_results[\"SVM\"] = print_results(y_test, clf.predict(x_test), mode, binary, debug)\n",
    "\n",
    "        clf = XGBRegressor(n_estimators=10, random_state=0)\n",
    "        clf.fit(x_train, y_train)\n",
    "        if debug:\n",
    "            print(\"XGBoost\")\n",
    "        prediction_results[\"XGBoost\"] = print_results(y_test, clf.predict(x_test), mode, binary, debug)\n",
    "        \n",
    "    metric = \"accuracy\"\n",
    "    highest_metric = -1\n",
    "    method = None\n",
    "    for key, value in prediction_results.items():\n",
    "        if value[metric] > highest_metric:\n",
    "            highest_metric = value[metric]\n",
    "            method = key\n",
    "    print(\"highest \", metric, \": \", method, \" \", highest_metric)\n",
    "        \n",
    "    return prediction_csv, prediction_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027761d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_csv = evaluate_one_round(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd9e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output prediction to csv file\n",
    "\n",
    "# pd_index = [\"y_id\"]\n",
    "# pd_data = np.expand_dims(np.array(np.array(y_id)[[i <= 80 for i in y_id]]), axis=-1)\n",
    "# for key, value in prediction_csv.items():\n",
    "#     pd_index.append(key)\n",
    "#     value = np.expand_dims(np.array(value, dtype=np.int32), axis=-1)\n",
    "#     pd_data = np.concatenate((pd_data, value), axis=1)\n",
    "    \n",
    "# pd_index = np.array(pd_index)\n",
    "\n",
    "# pd_prediction = pd.DataFrame(data=pd_data, columns=pd_index)\n",
    "\n",
    "# pd_prediction.to_csv(\"prediction(hsv_histogram_red).csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc5074a",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e08237",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"classification\":\n",
    "    x_img_train, y_img_train = x_img[[i > 80 for i in y_id]], y_img[[i > 80 for i in y_id]]\n",
    "    x_img_test, y_img_test = x_img[[i <= 80 for i in y_id]], y_img[[i <= 80 for i in y_id]]\n",
    "\n",
    "    predictions = np.argmax(clf.predict_proba(x_test), axis=1)\n",
    "    correct_index = predictions == y_test\n",
    "    for i, correct in enumerate(correct_index):\n",
    "        if correct:\n",
    "            correct_img = x_img_test[i]\n",
    "            correct_img = correct_img.astype(np.uint8)\n",
    "            cv2.putText(correct_img, str(y_test[i]), (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 1, cv2.LINE_AA)\n",
    "            cv2.imshow(\"correct\", correct_img)\n",
    "            cv2.waitKey(500)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a61c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"classification\":\n",
    "    wrong_index = predictions != y_test\n",
    "    for i, wrong in enumerate(wrong_index):\n",
    "        if wrong:\n",
    "            wrong_img = x_img_test[i]\n",
    "            wrong_img = wrong_img.astype(np.uint8)\n",
    "            cv2.putText(wrong_img, str(y_test[i]), (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 1, cv2.LINE_AA)\n",
    "            cv2.imshow(\"wrong\", wrong_img)\n",
    "            cv2.waitKey(500)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935cf15e",
   "metadata": {},
   "source": [
    "## K fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e62ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_list = np.arange(np.min(y_id), np.max(y_id)+1)\n",
    "# kf = KFold(n_splits=10)\n",
    "# #kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "# for i, (train_index, test_index) in enumerate(kf.split(id_list)):\n",
    "#     print(f\"Fold {i}:\")\n",
    "#     #print(f\"  Train: index={train_index}\")\n",
    "#     #print(f\"  Test:  index={test_index}\")\n",
    "#     train_id = id_list[train_index]\n",
    "#     test_id = id_list[test_index]\n",
    "#     #print(f\"  Train: index={train_id}\")\n",
    "#     #print(f\"  Test:  index={test_id}\")\n",
    "    \n",
    "#     x_train, y_train = x_final[train_id], y_final[train_id]\n",
    "#     x_test, y_test = x_final[test_id], y_final[test_id]\n",
    "    \n",
    "#     print(\"train: (0)\", np.sum(y_train==0), \"(1)\", np.sum(y_train==1))\n",
    "#     print(\"test: (0)\", np.sum(y_test==0), \"(1)\", np.sum(y_test==1))\n",
    "    \n",
    "#     prediction_csv = evaluate_one_round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5ffcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load y_id\n",
    "train_id = np.load(\"k fold (feature level)/train_id/train_id.npy\", allow_pickle=True)\n",
    "test_id = np.load(\"k fold (feature level)/test_id/test_id.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806bd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature(path):\n",
    "    feature = {\"id\": [], \"mean_H\": [], \"mean_S\": [], \"mean_V\":[], \"mean_R\": [], \"mean_G\": [], \"mean_B\": []}\n",
    "    \n",
    "    feature_pd = pd.read_csv(path)\n",
    "    feature_pd.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "    \n",
    "    for column in feature_pd.columns:\n",
    "        feature[column] = np.array(feature_pd[column].to_list())\n",
    "\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14267de",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_prediction_results = []\n",
    "\n",
    "feature_fusion = True\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(zip(train_id, test_id)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    #print(f\"  Train: index={train_index}\")\n",
    "    #print(f\"  Test:  index={test_index}\")\n",
    "    \n",
    "    train_idx, test_idx = [], []\n",
    "    for id in y_id:\n",
    "        if id in train_index:\n",
    "            train_idx.append(True)\n",
    "        else:\n",
    "            train_idx.append(False)\n",
    "            \n",
    "        if id in test_index:\n",
    "            test_idx.append(True)\n",
    "        else:\n",
    "            test_idx.append(False)\n",
    "\n",
    "    #print(f\"  Train: index={train_idx}\")\n",
    "    #print(f\"  Test:  index={test_idx}\")\n",
    "    \n",
    "    x_train, y_train = x_final[train_idx], y_final[train_idx]\n",
    "    x_test, y_test = x_final[test_idx], y_final[test_idx]\n",
    "    \n",
    "    if feature_fusion:\n",
    "        x_train_fusion, x_test_fusion = [], []\n",
    "        train_feature = load_feature(\"k fold (feature level)/k_fold_feature/train_feature_kfold_\"+str(i)+\".csv\")\n",
    "        test_feature = load_feature(\"k fold (feature level)/k_fold_feature/test_feature_kfold_\"+str(i)+\".csv\")\n",
    "        y_train_id = np.array(y_id)[train_idx]\n",
    "        y_test_id = np.array(y_id)[test_idx]\n",
    "        \n",
    "        #[]\n",
    "        #train_idx, test_idx = [], []\n",
    "        \n",
    "        for out_id, id_i in enumerate(y_train_id):\n",
    "#             if id_i in repeat:\n",
    "#                 train_idx.append(False)\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 train_idx.append(True)\n",
    "            for in_id, id_match in enumerate(train_feature[\"id\"]):\n",
    "                if id_i == id_match:\n",
    "                    repeat.append(id_match)\n",
    "                    x_train_fusion.append(np.concatenate((np.array(x_train[out_id]), \n",
    "                                                         np.array([train_feature[\"mean_G\"][in_id], train_feature[\"mean_B\"][in_id],\n",
    "                                                                  train_feature[\"mean_R\"][in_id], train_feature[\"mean_H\"][in_id],\n",
    "                                                                  train_feature[\"mean_S\"][in_id], train_feature[\"mean_V\"][in_id]])), axis=0))\n",
    "                    break\n",
    "        x_train_fusion = np.array(x_train_fusion)\n",
    "        #print(x_train_fusion.shape)\n",
    "        \n",
    "        for out_id, id_i in enumerate(y_test_id):\n",
    "#             if id_i in repeat:\n",
    "#                 test_idx.append(False)\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 test_idx.append(True)\n",
    "            for in_id, id_match in enumerate(test_feature[\"id\"]):\n",
    "                if id_i == id_match:\n",
    "                    repeat.append(id_match)\n",
    "                    x_test_fusion.append(np.concatenate((np.array(x_test[out_id]), \n",
    "                                                         np.array([test_feature[\"mean_G\"][in_id], test_feature[\"mean_B\"][in_id],\n",
    "                                                                  test_feature[\"mean_R\"][in_id], test_feature[\"mean_H\"][in_id],\n",
    "                                                                  test_feature[\"mean_S\"][in_id], test_feature[\"mean_V\"][in_id]])), axis=0))\n",
    "                    break\n",
    "        x_test_fusion = np.array(x_test_fusion)\n",
    "        #print(x_test_fusion.shape)\n",
    "        \n",
    "        # nail only\n",
    "        #y_train = y_train[train_idx]\n",
    "        #y_test = y_test[test_idx]\n",
    "    \n",
    "    print(\"train: (0)\", np.sum(y_train==0), \"(1)\", np.sum(y_train==1))\n",
    "    print(\"test: (0)\", np.sum(y_test==0), \"(1)\", np.sum(y_test==1))\n",
    "    \n",
    "    if feature_fusion:\n",
    "        x_train = x_train_fusion\n",
    "        x_test = x_test_fusion\n",
    "        \n",
    "#         for feature_i in range(x_train.shape[1]):\n",
    "#             if feature_i == 6 or feature_i == 7 or feature_i == 8:\n",
    "#                 continue\n",
    "#             x_train[:, feature_i] = x_train[:, feature_i] / 255.\n",
    "#             x_test[:, feature_i] = x_test[:, feature_i] / 255.\n",
    "        \n",
    "#     scaler = StandardScaler()\n",
    "#     x_train = scaler.fit_transform(x_train)\n",
    "#     x_test = scaler.transform(x_test)\n",
    "    \n",
    "    #np.savez(\"feature_fusion\"+str(i)+\".npz\", x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)\n",
    "        \n",
    "#     if mode == \"classification\":\n",
    "#         if binary:\n",
    "#             plt.plot(np.arange(x_train.shape[1]), x_train[y_train==0].mean(axis=0), label='non-anemia')\n",
    "#             plt.fill_between(np.arange(x_train.shape[1]), x_train[y_train==0].mean(axis=0)-x_train[y_train==0].std(axis=0), x_train[y_train==0].mean(axis=0)+x_train[y_train==0].std(axis=0), alpha=0.4)\n",
    "#             plt.plot(np.arange(x_train.shape[1]), x_train[y_train==1].mean(axis=0), label='anemia')\n",
    "#             plt.fill_between(np.arange(x_train.shape[1]), x_train[y_train==1].mean(axis=0)-x_train[y_train==1].std(axis=0), x_train[y_train==1].mean(axis=0)+x_train[y_train==1].std(axis=0), alpha=0.4)\n",
    "#     plt.title(\"Fold \" + str(i) + \" (Train)\")\n",
    "#     plt.xlabel(\"Eyelid (GBRHSV), Nail (GBRHSV)\")\n",
    "#     plt.ylabel(\"Value\")\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "    \n",
    "#     if mode == \"classification\":\n",
    "#         if binary:\n",
    "#             plt.plot(np.arange(x_test.shape[1]), x_test[y_test==0].mean(axis=0), label='non-anemia')\n",
    "#             plt.fill_between(np.arange(x_test.shape[1]), x_test[y_test==0].mean(axis=0)-x_test[y_test==0].std(axis=0), x_test[y_test==0].mean(axis=0)+x_test[y_test==0].std(axis=0), alpha=0.4)\n",
    "#             plt.plot(np.arange(x_test.shape[1]), x_test[y_test==1].mean(axis=0), label='anemia')\n",
    "#             plt.fill_between(np.arange(x_test.shape[1]), x_test[y_test==1].mean(axis=0)-x_test[y_test==1].std(axis=0), x_test[y_test==1].mean(axis=0)+x_test[y_test==1].std(axis=0), alpha=0.4)\n",
    "#     plt.title(\"Fold \" + str(i) + \" (Test)\")\n",
    "#     plt.xlabel(\"Eyelid (GBRHSV), Nail (GBRHSV)\")\n",
    "#     plt.ylabel(\"Value\")\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "    \n",
    "    prediction_csv, prediction_results = evaluate_one_round(x_train, y_train, x_test, y_test, False)\n",
    "    \n",
    "    kfold_prediction_results.append(prediction_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e3ff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for i in kfold_prediction_results:\n",
    "    for key, value in i.items():\n",
    "        if not results.get(key):\n",
    "            results[key] = []\n",
    "        results[key].append(value[\"accuracy\"])\n",
    "        \n",
    "for method, accuracy in results.items():\n",
    "    print(method, \": \", np.mean(accuracy), np.std(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de5302d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d0ae51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
